{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio \n",
    "import librispeech \n",
    "import my_functions\n",
    "import torch_functions \n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lib = librispeech.LIBRISPEECH('/Users/stephen/code/projects/Speech_Recognition/Data/', url=\"train-clean-100\", download=True)\n",
    "test_lib = librispeech.LIBRISPEECH('/Users/stephen/code/projects/Speech_Recognition/Data/', url=\"test-clean\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training length: 28539\n",
      "Test length: 2620\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training length: {len(train_lib)}\")\n",
    "print(f\"Test length: {len(test_lib)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "    def __init__(self):\n",
    "        char_map_str = \"\"\"\n",
    "        ' 0\n",
    "        <SPACE> 1\n",
    "        a 2\n",
    "        b 3\n",
    "        c 4\n",
    "        d 5\n",
    "        e 6\n",
    "        f 7\n",
    "        g 8\n",
    "        h 9\n",
    "        i 10\n",
    "        j 11\n",
    "        k 12\n",
    "        l 13\n",
    "        m 14\n",
    "        n 15\n",
    "        o 16\n",
    "        p 17\n",
    "        q 18\n",
    "        r 19\n",
    "        s 20\n",
    "        t 21\n",
    "        u 22\n",
    "        v 23\n",
    "        w 24\n",
    "        x 25\n",
    "        y 26\n",
    "        z 27\n",
    "        \"\"\"\n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        for line in char_map_str.strip().split('\\n'):\n",
    "            ch, index = line.split()\n",
    "            self.char_map[ch] = int(index)\n",
    "            self.index_map[int(index)] = ch\n",
    "        self.index_map[1] = ' '\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c == ' ':\n",
    "                ch = self.char_map['<SPACE>']\n",
    "            else:\n",
    "                ch = self.char_map[c]\n",
    "            int_sequence.append(ch)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[i])\n",
    "        return ''.join(string).replace('<SPACE>', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform = TextTransform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transform and augmentation\n",
    "SAMPLE_RATE = 8000 \n",
    "WINDOW_LENGTH = int(0.020 * SAMPLE_RATE)  # 25ms windows \n",
    "HOP_LENGTH = int(0.01 * SAMPLE_RATE) # 10ms sliding overlapping window \n",
    "N_MELS = 80\n",
    "DURATION = 16.7  # duration in seconds \n",
    "N_SAMPLES = int(SAMPLE_RATE * DURATION)\n",
    "N_FFT = 400 \n",
    "\n",
    "\n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.Spectrogram(n_fft=N_FFT),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=80),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.Spectrogram(n_fft=N_FFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, _, utterance, _, _, _) in data: \n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == 'valid': \n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else: \n",
    "            raise Exception('data_type should be train or valid')\n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
    "\targ_maxes = torch.argmax(output, dim=2)\n",
    "\tdecodes = []\n",
    "\ttargets = []\n",
    "\tfor i, args in enumerate(arg_maxes):\n",
    "\t\tdecode = []\n",
    "\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "\t\tfor j, index in enumerate(args):\n",
    "\t\t\tif index != blank_label:\n",
    "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tdecode.append(index.item())\n",
    "\t\tdecodes.append(text_transform.int_to_text(decode))\n",
    "\treturn decodes, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "based of DeepSpeech\n",
    "https://pytorch.org/audio/stable/_modules/torchaudio/models/deepspeech.html#DeepSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\"DeepSpeech\"]\n",
    "\n",
    "\n",
    "class FullyConnected(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        n_feature: Number of input features\n",
    "        n_hidden: Internal hidden unit size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_feature: int,\n",
    "                 n_hidden: int,\n",
    "                 dropout: float,\n",
    "                 relu_max_clip: int = 20) -> None:\n",
    "        super(FullyConnected, self).__init__()\n",
    "        self.fc = torch.nn.Linear(n_feature, n_hidden, bias=True)\n",
    "        self.relu_max_clip = relu_max_clip\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.hardtanh(x, 0, self.relu_max_clip)\n",
    "        if self.dropout:\n",
    "            x = torch.nn.functional.dropout(x, self.dropout, self.training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSpeech(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepSpeech model architecture from *Deep Speech: Scaling up end-to-end speech recognition* \n",
    "    [:footcite: 'hannun2014deep'].\n",
    "\n",
    "    Args:\n",
    "        n_features: Number of input features\n",
    "        n_hidden: Internal hidden unit size.\n",
    "        n_class: Number of output classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_feature: int, \n",
    "        n_hidden: int = 2048, \n",
    "        n_class: int = 40, \n",
    "        dropout: float = 0.0\n",
    "    ) -> None:\n",
    "        super(DeepSpeech, self).__init__()\n",
    "        self.N_hidden = n_hidden\n",
    "        self.fc1 = FullyConnected(n_feature, n_hidden, dropout)\n",
    "        self.fc2 = FullyConnected(n_feature, n_hidden, dropout)\n",
    "        self.fc3 = FullyConnected(n_feature, n_hidden, dropout)\n",
    "        self.bi_rnn = nn.RNN(\n",
    "            n_hidden, n_hidden, num_layers=1, nonlinearity=\"relu\", bidirectional=True\n",
    "        )\n",
    "        self.fc4 = FullyConnected(n_hidden, n_hidden, dropout)\n",
    "        self.out = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                x (torch.Tensor): Tensor of dimension (batch, channel, time, feature).\n",
    "            Returns:\n",
    "                Tensor: Predictor tensor of dimension (batch, time, class).\n",
    "            \"\"\"\n",
    "            # N x C x T x F\n",
    "            x = self.fc1(x)\n",
    "            # N x C x T x F\n",
    "            x = self.fc2(x)\n",
    "            # N x C x T x F\n",
    "            x = self.fc3(x)\n",
    "            # N x C x T x F\n",
    "            x = x.squeeze(1)\n",
    "            # N x T x H \n",
    "            x = x.transpose(0, 1)\n",
    "            # T x N x H \n",
    "            x, _ = self.bi_rnn(x)\n",
    "            # The fifth (non-recurrent) layer takes both the forward and backward units as inputs \n",
    "            x = x[:, :, :self.n_hidden] + x[:, :, self.n_hidden:]\n",
    "            # T x N xH \n",
    "            x = self.fc4(x)\n",
    "            # T x N x H\n",
    "            x = self.out(x)\n",
    "            # T x N x n_class \n",
    "            x = x.permute(1, 0, 2)\n",
    "            # N x T x n_class \n",
    "            x = nn.functional.log_softmax(x, dim=2)\n",
    "            # N x T x n_class \n",
    "            return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea3e9d44676e53d067b20c1e2b0aacc847e3e53ef51186a02d87f52450540b7c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('audio_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
