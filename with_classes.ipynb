{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio \n",
    "import librispeech \n",
    "import my_functions\n",
    "import torch_functions \n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lib = librispeech.LIBRISPEECH('/Users/stephen/code/projects/Speech_Recognition/Data/', url=\"train-clean-100\", download=True)\n",
    "test_lib = librispeech.LIBRISPEECH('/Users/stephen/code/projects/Speech_Recognition/Data/', url=\"test-clean\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training length: 28539\n",
      "Test length: 2620\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training length: {len(train_lib)}\")\n",
    "print(f\"Test length: {len(test_lib)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "    def __init__(self):\n",
    "        char_map_str = \"\"\"\n",
    "        ' 0\n",
    "        <SPACE> 1\n",
    "        a 2\n",
    "        b 3\n",
    "        c 4\n",
    "        d 5\n",
    "        e 6\n",
    "        f 7\n",
    "        g 8\n",
    "        h 9\n",
    "        i 10\n",
    "        j 11\n",
    "        k 12\n",
    "        l 13\n",
    "        m 14\n",
    "        n 15\n",
    "        o 16\n",
    "        p 17\n",
    "        q 18\n",
    "        r 19\n",
    "        s 20\n",
    "        t 21\n",
    "        u 22\n",
    "        v 23\n",
    "        w 24\n",
    "        x 25\n",
    "        y 26\n",
    "        z 27\n",
    "        \"\"\"\n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        for line in char_map_str.strip().split('\\n'):\n",
    "            ch, index = line.split()\n",
    "            self.char_map[ch] = int(index)\n",
    "            self.index_map[int(index)] = ch\n",
    "        self.index_map[1] = ' '\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c == ' ':\n",
    "                ch = self.char_map['<SPACE>']\n",
    "            else:\n",
    "                ch = self.char_map[c]\n",
    "            int_sequence.append(ch)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[i])\n",
    "        return ''.join(string).replace('<SPACE>', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform = TextTransform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transform and augmentation\n",
    "#SAMPLE_RATE = 8000 \n",
    "#WINDOW_LENGTH = int(0.020 * SAMPLE_RATE)  # 25ms windows \n",
    "#HOP_LENGTH = int(0.01 * SAMPLE_RATE) # 10ms sliding overlapping window \n",
    "#N_MELS = 80\n",
    "#DURATION = 16.7  # duration in seconds \n",
    "#N_SAMPLES = int(SAMPLE_RATE * DURATION)\n",
    "#N_FFT = 400 \n",
    "n_fft = 1024 \n",
    "win_length = None \n",
    "hop_length = 512 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.Spectrogram(n_fft=n_fft, \n",
    "    win_length=win_length, \n",
    "    hop_length=hop_length, \n",
    "    center=-True,\n",
    "    pad_mode=\"reflect\", \n",
    "    power=2.0),\n",
    "    torchaudio.transforms.TimeStretch(fixed_rate=1.2),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=80),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.Spectrogram(n_fft=n_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, _, utterance, _, _, _) in data: \n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == 'valid': \n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else: \n",
    "            raise Exception('data_type should be train or valid')\n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
    "\t\"\"\"outputs the most probable character at each time step.\n",
    "\n",
    "\tArgs:\n",
    "\t\toutput (_type_): _description_\n",
    "\t\tlabels (_type_): _description_\n",
    "\t\tlabel_lengths (_type_): _description_\n",
    "\t\tblank_label (int, optional): _description_. Defaults to 28.\n",
    "\t\tcollapse_repeated (bool, optional): _description_. Defaults to True.\n",
    "\n",
    "\tReturns:\n",
    "\t\t_type_: _description_\n",
    "\t\"\"\"\n",
    "\targ_maxes = torch.argmax(output, dim=2)\n",
    "\tdecodes = []\n",
    "\ttargets = []\n",
    "\tfor i, args in enumerate(arg_maxes):\n",
    "\t\tdecode = []\n",
    "\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "\t\tfor j, index in enumerate(args):\n",
    "\t\t\tif index != blank_label:\n",
    "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tdecode.append(index.item())\n",
    "\t\tdecodes.append(text_transform.int_to_text(decode))\n",
    "\treturn decodes, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "based of DeepSpeech2\n",
    "link: https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition/deepspeech2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea3e9d44676e53d067b20c1e2b0aacc847e3e53ef51186a02d87f52450540b7c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('audio_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
