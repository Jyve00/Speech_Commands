{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotional Speech Recognition\n",
    "This notebook will be an audio classification problem and solved with Audio Feature extraction and augmentation, Machine Learning and Deep Learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/dejolilandry/asvpesdspeech-nonspeech-emotional-utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os \n",
    "import math\n",
    "import torch \n",
    "import torchaudio \n",
    "from torch.utils.data import Dataset, dataloader\n",
    "import matplotlib.pyplot as plt \n",
    "#%matplotlib_inline\n",
    "import seaborn as sns\n",
    "import librosa \n",
    "import librosa.display\n",
    "\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation \n",
    "Since the dataset didnt come with a CSV file containing the metadata we'll create a dataframe that contains the Path to the Wav file and it Class which is what emotion it has been labeled as. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from the Kaggle Dataset Descri\n",
    "\n",
    "Each wav file in the dataset consists of numerical identifiers. \n",
    "\n",
    "Filename identifiers:\n",
    "Modality ( 03 = audio-only).\n",
    "Vocal channel on s(01 = speech, 02 = npeech).\n",
    "Emotion ( 01 = boredom, 02 = neutral, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised, 09 = excited, 10 = pleasure, 11 = pain, 12 = disappointment, 13 = others).\n",
    "Emotional intensity (01 = normal, 02 = high).\n",
    "Statement (as itâ€™s non scripted this refer to the number of sample select per actor folder ).\n",
    "Actor ( even numbered acteurs are male, odd numbered actors are female).\n",
    "Age(01 = above 65, 02 = between 20~64, 03 = under 20,04=new born).\n",
    "Source of downloading (01 =website , 02 = youtube channel, 03= movies).\n",
    "Language(01=Chinese , 02=English ,04 = french , others).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {\n",
    "    1: 'boredom', \n",
    "    2: 'neutral',\n",
    "    3: 'happy',\n",
    "    4: 'sad',\n",
    "    5: 'angry',\n",
    "    6: 'fearful',\n",
    "    7: 'disgust', \n",
    "    8: 'surprised',\n",
    "    9: 'excited', \n",
    "    10: 'pleasure',\n",
    "    11: 'pain', \n",
    "    12: 'disappointment', \n",
    "    13: 'others'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by taking a look at how the data is labeled we can decide how to make classification for our problem. Lets start simple and just use the Emotion label. Later we can try classifying other labels like age and gender. We'll also only use files labeled as speech, later we'll use the non speech files for data augmentation but more on that later. for now lets make a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['03', '01', '05', '01', '07', '103', '02', '01', '02', '18']\n"
     ]
    }
   ],
   "source": [
    "# lets practice taking the path strings apart \n",
    "ster = \"/Users/stephen/Desktop/Speech_Commands/Data/ASVP-ESD_UPDATE/Audio/actor_103/03-01-05-01-07-103-02-01-02-18.wav\"\n",
    "part = ster.split('/')[-1]\n",
    "part = part.split('.')[0]\n",
    "part = part.split('-')\n",
    "print(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "      <th>Folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>/03-01-02-01-02-16-03-03-01.wav</td>\n",
       "      <td>actor_16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry</td>\n",
       "      <td>/03-01-05-02-07-16-03-03-01.wav</td>\n",
       "      <td>actor_16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>/03-01-02-01-13-16-03-03-01.wav</td>\n",
       "      <td>actor_16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>/03-01-02-01-05-16-03-03-01.wav</td>\n",
       "      <td>actor_16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>/03-01-05-02-06-16-03-03-01.wav</td>\n",
       "      <td>actor_16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                             Path    Folder\n",
       "0  neutral  /03-01-02-01-02-16-03-03-01.wav  actor_16\n",
       "1    angry  /03-01-05-02-07-16-03-03-01.wav  actor_16\n",
       "2  neutral  /03-01-02-01-13-16-03-03-01.wav  actor_16\n",
       "3  neutral  /03-01-02-01-05-16-03-03-01.wav  actor_16\n",
       "4    angry  /03-01-05-02-06-16-03-03-01.wav  actor_16"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# string representing PATH for folder containing data\n",
    "audio_path = '/Users/stephen/Desktop/Speech_Commands/Data/ASVP-ESD_UPDATE/Audio/'\n",
    "\n",
    "# the dataset is organized into folders for each actor/actress. the listdir method will list all contained folders \n",
    "dir_list = os.listdir(audio_path)\n",
    "\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "file_fold = []\n",
    "\n",
    "# iterate through files and \n",
    "for dir in dir_list:\n",
    "    # extract contents from each folder \n",
    "    actor = os.listdir(audio_path + dir)\n",
    "    # go through each file in each folder \n",
    "    for file in actor: \n",
    "        part = file.split('.')[0] # ignore the .wav\n",
    "        part = part.split('-')\n",
    "        # the second part represents speech or non speech \n",
    "        if part[1] == '01': # 01 represents Speech \n",
    "            # third part represents emotion \n",
    "            file_emotion.append(int(part[2]))\n",
    "            file_path.append('/' + file)\n",
    "            file_fold.append(dir)\n",
    "\n",
    "\n",
    "\n",
    "# dataframe for emotion of files \n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files \n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "\n",
    "# dataframe for names of folders \n",
    "fold_df = pd.DataFrame(file_fold, columns=['Folder'])\n",
    "\n",
    "metadata = pd.concat([emotion_df, path_df, fold_df], axis=1)\n",
    "\n",
    "# change integers to actual emotions. \n",
    "metadata.Emotions.replace(labels_dict, inplace=True)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/03-01-02-01-02-16-03-03-01.wav'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check to make sure the Path Column contains the fill path \n",
    "metadata['Path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save to CSV file for later use. The PandasData frame will be used to visualize some data \n",
    "metadata.to_csv('/Users/stephen/Desktop/Speech_Commands/Data/ASVP-ESD_UPDATE/metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization \n",
    "lets makes some visuals \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral           1019\n",
       "angry              987\n",
       "happy              495\n",
       "sad                471\n",
       "surprised          327\n",
       "disappointment     241\n",
       "fearful            137\n",
       "disgust             93\n",
       "excited             92\n",
       "pain                36\n",
       "boredom             26\n",
       "pleasure            22\n",
       "Name: Emotions, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata['Emotions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephen/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Count of Emotions', size=20)\n",
    "sns.countplot(metadata.Emotions)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/stephen/Desktop/Speech_Commands/Data/ASVP-ESD_UPDATE/Audio/'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'actor_16'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.read_csv('/Users/stephen/Desktop/Speech_Commands/Data/ASVP-ESD_UPDATE/metadata.csv')\n",
    "d.iloc[0, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprossing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not that we are able to locate our data properly we'll create a custom Dataset object with Pytorch. This will make it easier to work with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Dataset class \n",
    "\n",
    "class EmotionalSpeechDataset(Dataset):\n",
    "    # constructor \n",
    "    def __init__(self, annotations_file, audio_dir, target_sample_rate, num_samples, device, transformation=None): \n",
    "        self.annotations_file = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir \n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples \n",
    "        self.device = device \n",
    "        self.transformation = transformation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations_file)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "\n",
    "        # if a transformation is stated, apply to audio tensor \n",
    "        if self.transformation: \n",
    "            signal = self.transformation(signal).to(self.device)\n",
    "        return signal, label \n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal \n",
    "\n",
    "    def _right_pad_if_necessary(self, signal): \n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:    # compare each file with desired length \n",
    "            num_missing_samples = self.num_samples - length_signal \n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal \n",
    "\n",
    "    def _resample_if_necessary(self, signal):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal \n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):  # if file is Stereo, convert to mono by using the mean \n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal \n",
    "\n",
    "    def _get_audio_path(self, index):\n",
    "        fold = f\"fold{self.annotations.iloc[index, 2]}\"    # grab from \"Folder\" column \n",
    "        path = os.path.join(self.audio_dir, fold, self.annotations_file.iloc[index, 1]) # combine to make full PATH\n",
    "        return path \n",
    "    \n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 0]  # grab from \"label\" column  \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/Users/stephen/Desktop/Speech_Commands/Data/ASVP-ESD_UPDATE/metadata.csv/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m3/ywsrz7k170vbk8511l1h_6840000gn/T/ipykernel_76572/4091285238.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m emotional_speech_dataset = EmotionalSpeechDataset(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mannotations_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/Users/stephen/Desktop/Speech_Commands/Data/ASVP-ESD_UPDATE/metadata.csv/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0maudio_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m3/ywsrz7k170vbk8511l1h_6840000gn/T/ipykernel_76572/2084546102.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, annotations_file, audio_dir, target_sample_rate, num_samples, device, transformation)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# constructor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_sample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_sample_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/Users/stephen/Desktop/Speech_Commands/Data/ASVP-ESD_UPDATE/metadata.csv/'"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "emotional_speech_dataset = EmotionalSpeechDataset(\n",
    "    annotations_file='/Users/stephen/Desktop/Speech_Commands/Data/ASVP-ESD_UPDATE/metadata.csv/',\n",
    "    audio_dir=audio_path,\n",
    "    target_sample_rate=16000,\n",
    "    num_samples=16000*20, \n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3946"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emotional_speech_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m3/ywsrz7k170vbk8511l1h_6840000gn/T/ipykernel_76572/3535156590.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memotional_speech_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/m3/ywsrz7k170vbk8511l1h_6840000gn/T/ipykernel_76572/2605281212.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0maudio_sample_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_audio_sample_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_audio_sample_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_sample_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute_name)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "emotional_speech_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3358073929bd3d27ff594bc6528257efc4213b34ba9d7c6bd240dce3a23a83d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('torch_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
