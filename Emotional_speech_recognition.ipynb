{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotional Speech Recognition\n",
    "This notebook will be an audio classification problem and solved with Audio Feature extraction and augmentation, Machine Learning and Deep Learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/dejolilandry/asvpesdspeech-nonspeech-emotional-utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os \n",
    "import math\n",
    "import torch \n",
    "import torchaudio \n",
    "import matplotlib.pyplot as plt \n",
    "#%matplotlib_inline\n",
    "import seaborn as sns\n",
    "import librosa \n",
    "import librosa.display\n",
    "\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation \n",
    "Since the dataset didnt come with a CSV file containing the metadata we'll create a dataframe that contains the Path to the Wav file and it Class which is what emotion it has been labeled as. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from the Kaggle Dataset Descri\n",
    "\n",
    "Each wav file in the dataset consists of numerical identifiers. \n",
    "\n",
    "Filename identifiers:\n",
    "Modality ( 03 = audio-only).\n",
    "Vocal channel on s(01 = speech, 02 = npeech).\n",
    "Emotion ( 01 = boredom, 02 = neutral, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised, 09 = excited, 10 = pleasure, 11 = pain, 12 = disappointment, 13 = others).\n",
    "Emotional intensity (01 = normal, 02 = high).\n",
    "Statement (as itâ€™s non scripted this refer to the number of sample select per actor folder ).\n",
    "Actor ( even numbered acteurs are male, odd numbered actors are female).\n",
    "Age(01 = above 65, 02 = between 20~64, 03 = under 20,04=new born).\n",
    "Source of downloading (01 =website , 02 = youtube channel, 03= movies).\n",
    "Language(01=Chinese , 02=English ,04 = french , others).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {\n",
    "    1: 'boredom', \n",
    "    2: 'neutral',\n",
    "    3: 'happy',\n",
    "    4: 'sad',\n",
    "    5: 'angry',\n",
    "    6: 'fearful',\n",
    "    7: 'disgust', \n",
    "    8: 'surprised',\n",
    "    9: 'excited', \n",
    "    10: 'pleasure',\n",
    "    11: 'pain', \n",
    "    12: 'disappointment', \n",
    "    13: 'others'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by taking a look at how the data is labeled we can decide how to make classification for our problem. Lets start simple and just use the Emotion label. Later we can try classifying other labels like age and gender. We'll also only use files labeled as speech, later we'll use the non speech files for data augmentation but more on that later. for now lets make a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['03', '01', '05', '01', '07', '103', '02', '01', '02', '18']\n"
     ]
    }
   ],
   "source": [
    "# lets practice taking the path strings apart \n",
    "ster = \"/Users/stephen/Desktop/Speech_Commands/Data/ASVP-ESD_UPDATE/Audio/actor_103/03-01-05-01-07-103-02-01-02-18.wav\"\n",
    "part = ster.split('/')[-1]\n",
    "part = part.split('.')[0]\n",
    "part = part.split('-')\n",
    "print(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>/Users/stephen/Desktop/Speech_Commands/Data/AS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry</td>\n",
       "      <td>/Users/stephen/Desktop/Speech_Commands/Data/AS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>/Users/stephen/Desktop/Speech_Commands/Data/AS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>/Users/stephen/Desktop/Speech_Commands/Data/AS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>/Users/stephen/Desktop/Speech_Commands/Data/AS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                                               Path\n",
       "0  neutral  /Users/stephen/Desktop/Speech_Commands/Data/AS...\n",
       "1    angry  /Users/stephen/Desktop/Speech_Commands/Data/AS...\n",
       "2  neutral  /Users/stephen/Desktop/Speech_Commands/Data/AS...\n",
       "3  neutral  /Users/stephen/Desktop/Speech_Commands/Data/AS...\n",
       "4    angry  /Users/stephen/Desktop/Speech_Commands/Data/AS..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# string representing PATH for folder containing data\n",
    "audio_path = '/Users/stephen/Desktop/Speech_Commands/Data/ASVP-ESD_UPDATE/Audio/'\n",
    "\n",
    "# the dataset is organized into folders for each actor/actress. the listdir method will list all contained folders \n",
    "dir_list = os.listdir(audio_path)\n",
    "\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "# iterate through files and \n",
    "for dir in dir_list:\n",
    "    # extract contents from each folder \n",
    "    actor = os.listdir(audio_path + dir)\n",
    "    # go through each file in each folder \n",
    "    for file in actor: \n",
    "        part = file.split('.')[0] # ignore the .wav\n",
    "        part = part.split('-') \n",
    "        # the second part represents speech or non speech \n",
    "        if part[1] == '01': # 01 represents Speech \n",
    "            # third part represents emotion \n",
    "            file_emotion.append(int(part[2]))\n",
    "            file_path.append(audio_path + dir + '/' + file)\n",
    "\n",
    "\n",
    "\n",
    "# dataframe for emotion of files \n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files \n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "metadata = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# change integers to actual emotions. \n",
    "metadata.Emotions.replace(labels_dict, inplace=True)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/stephen/Desktop/Speech_Commands/Data/ASVP-ESD_UPDATE/Audio/actor_16/03-01-02-01-02-16-03-03-01.wav'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check to make sure the Path Column contains the fill path \n",
    "metadata['Path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save to CSV file for later use. The PandasData frame will be used to visualize some data \n",
    "metadata.to_csv('/Users/stephen/Desktop/Speech_Commands/Data/ASVP-ESD_UPDATE/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3358073929bd3d27ff594bc6528257efc4213b34ba9d7c6bd240dce3a23a83d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('torch_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
