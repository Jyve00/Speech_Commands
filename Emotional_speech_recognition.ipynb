{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotional Speech Recognition\n",
    "This notebook will be an audio classification problem and solved with Audio Feature extraction and augmentation, Machine Learning and Deep Learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/dejolilandry/asvpesdspeech-nonspeech-emotional-utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os \n",
    "import math\n",
    "import torch \n",
    "import torchaudio \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt \n",
    "#%matplotlib_inline\n",
    "import seaborn as sns\n",
    "import librosa \n",
    "import librosa.display\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "# custom modules \n",
    "from MyDataClasses import ASVPDataset\n",
    "import torch_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Sample_Rate</th>\n",
       "      <th>Num_Frames</th>\n",
       "      <th>Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>actor_16</td>\n",
       "      <td>/03-01-02-01-02-16-03-03-01.wav</td>\n",
       "      <td>16000</td>\n",
       "      <td>44305</td>\n",
       "      <td>2.769062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry</td>\n",
       "      <td>actor_16</td>\n",
       "      <td>/03-01-05-02-07-16-03-03-01.wav</td>\n",
       "      <td>16000</td>\n",
       "      <td>96922</td>\n",
       "      <td>6.057625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>actor_16</td>\n",
       "      <td>/03-01-02-01-13-16-03-03-01.wav</td>\n",
       "      <td>16000</td>\n",
       "      <td>58396</td>\n",
       "      <td>3.649750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>actor_16</td>\n",
       "      <td>/03-01-02-01-05-16-03-03-01.wav</td>\n",
       "      <td>16000</td>\n",
       "      <td>13794</td>\n",
       "      <td>0.862125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>actor_16</td>\n",
       "      <td>/03-01-05-02-06-16-03-03-01.wav</td>\n",
       "      <td>16000</td>\n",
       "      <td>49738</td>\n",
       "      <td>3.108625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions    Folder                         Filename  Sample_Rate  \\\n",
       "0  neutral  actor_16  /03-01-02-01-02-16-03-03-01.wav        16000   \n",
       "1    angry  actor_16  /03-01-05-02-07-16-03-03-01.wav        16000   \n",
       "2  neutral  actor_16  /03-01-02-01-13-16-03-03-01.wav        16000   \n",
       "3  neutral  actor_16  /03-01-02-01-05-16-03-03-01.wav        16000   \n",
       "4    angry  actor_16  /03-01-05-02-06-16-03-03-01.wav        16000   \n",
       "\n",
       "   Num_Frames  Duration  \n",
       "0       44305  2.769062  \n",
       "1       96922  6.057625  \n",
       "2       58396  3.649750  \n",
       "3       13794  0.862125  \n",
       "4       49738  3.108625  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ASVP_dir = '/Users/stephen/Desktop/Speech_Recognition/Data/ASVP-ESD_UPDATE/Audio/'\n",
    "ASVP_metadata = pd.read_csv('/Users/stephen/Desktop/Speech_Recognition/Data/ASVP-ESD_UPDATE/asvp_metadata.csv', index_col=0)\n",
    "ASVP_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the dataset is: 3946\n"
     ]
    }
   ],
   "source": [
    "print(f'The length of the dataset is: {len(ASVP_metadata)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_Rate</th>\n",
       "      <th>Num_Frames</th>\n",
       "      <th>Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3946.0</td>\n",
       "      <td>3946.000000</td>\n",
       "      <td>3946.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>100414.223517</td>\n",
       "      <td>6.275889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>57767.822831</td>\n",
       "      <td>3.610489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>6069.000000</td>\n",
       "      <td>0.379312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>51052.250000</td>\n",
       "      <td>3.190766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>96000.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>146401.000000</td>\n",
       "      <td>9.150063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>489013.000000</td>\n",
       "      <td>30.563312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sample_Rate     Num_Frames     Duration\n",
       "count       3946.0    3946.000000  3946.000000\n",
       "mean       16000.0  100414.223517     6.275889\n",
       "std            0.0   57767.822831     3.610489\n",
       "min        16000.0    6069.000000     0.379312\n",
       "25%        16000.0   51052.250000     3.190766\n",
       "50%        16000.0   96000.000000     6.000000\n",
       "75%        16000.0  146401.000000     9.150063\n",
       "max        16000.0  489013.000000    30.563312"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ASVP_metadata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deciding how long all the files will be \n",
    "inorder for the CNN to take in an audio dataset all the audio files must be the same length. From the analysis we did in EDA we can see the lengths vary from 3.7 seconds to over 330 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17 rows over 20 seconds\n"
     ]
    }
   ],
   "source": [
    "# how many are less than 20 seconds \n",
    "over_20_sec = len(ASVP_metadata) - len(ASVP_metadata[ASVP_metadata['Duration'] <= 20])\n",
    "print(f\"There are {over_20_sec} rows over 20 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we only lose 17 rows if we get ride of everything over 20 seconds. Lets see how many we lose when we cut everything over 15 seconds and 10 seconds. The shorter then length of all our audio files the less computation power we'll nee. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2.769062\n",
       "1        6.057625\n",
       "2        3.649750\n",
       "3        0.862125\n",
       "4        3.108625\n",
       "          ...    \n",
       "3941    10.488000\n",
       "3942    10.440000\n",
       "3943    10.392000\n",
       "3944    12.408000\n",
       "3945     9.743688\n",
       "Name: Duration, Length: 3946, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ASVP_metadata['Duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Sample_Rate</th>\n",
       "      <th>Num_Frames</th>\n",
       "      <th>Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>actor_16</td>\n",
       "      <td>/03-01-02-01-02-16-03-03-01.wav</td>\n",
       "      <td>16000</td>\n",
       "      <td>44305</td>\n",
       "      <td>2.769062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry</td>\n",
       "      <td>actor_16</td>\n",
       "      <td>/03-01-05-02-07-16-03-03-01.wav</td>\n",
       "      <td>16000</td>\n",
       "      <td>96922</td>\n",
       "      <td>6.057625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>actor_16</td>\n",
       "      <td>/03-01-02-01-13-16-03-03-01.wav</td>\n",
       "      <td>16000</td>\n",
       "      <td>58396</td>\n",
       "      <td>3.649750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>actor_16</td>\n",
       "      <td>/03-01-02-01-05-16-03-03-01.wav</td>\n",
       "      <td>16000</td>\n",
       "      <td>13794</td>\n",
       "      <td>0.862125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>actor_16</td>\n",
       "      <td>/03-01-05-02-06-16-03-03-01.wav</td>\n",
       "      <td>16000</td>\n",
       "      <td>49738</td>\n",
       "      <td>3.108625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions    Folder                         Filename  Sample_Rate  \\\n",
       "0  neutral  actor_16  /03-01-02-01-02-16-03-03-01.wav        16000   \n",
       "1    angry  actor_16  /03-01-05-02-07-16-03-03-01.wav        16000   \n",
       "2  neutral  actor_16  /03-01-02-01-13-16-03-03-01.wav        16000   \n",
       "3  neutral  actor_16  /03-01-02-01-05-16-03-03-01.wav        16000   \n",
       "4    angry  actor_16  /03-01-05-02-06-16-03-03-01.wav        16000   \n",
       "\n",
       "   Num_Frames  Duration  \n",
       "0       44305  2.769062  \n",
       "1       96922  6.057625  \n",
       "2       58396  3.649750  \n",
       "3       13794  0.862125  \n",
       "4       49738  3.108625  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global Variables \n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "N_FFT = int(0.025 * SAMPLE_RATE)   # 25 ms \n",
    "HOP_LENGTH = int(0.01 * SAMPLE_RATE)  # 10 ms\n",
    "DURATION = 20 \n",
    "N_SAMPLES = SAMPLE_RATE * DURATION \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataset = ASVPDataset(\n",
    "    annotations_file=ASVP_metadata, \n",
    "    audio_dir=ASVP_dir,\n",
    "    target_sample_rate=SAMPLE_RATE, \n",
    "    num_samples=N_SAMPLES,\n",
    "    device=device  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3946"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.]]), 'angry')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_dataset[133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephen/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Count of Emotions', size=20)\n",
    "sns.countplot(ASVP_metadata.Emotions)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprossing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not that we are able to locate our data properly we'll create a custom Dataset object with Pytorch. This will make it easier to work with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions \n",
    "\n",
    "def print_stats(waveform, sample_rate=None, src=None):\n",
    "  if src:\n",
    "    print(\"-\" * 10)\n",
    "    print(\"Source:\", src)\n",
    "    print(\"-\" * 10)\n",
    "  if sample_rate:\n",
    "    print(\"Sample Rate:\", sample_rate)\n",
    "  print(\"Shape:\", tuple(waveform.shape))\n",
    "  print(\"Dtype:\", waveform.dtype)\n",
    "  print(f\" - Max:     {waveform.max().item():6.3f}\")\n",
    "  print(f\" - Min:     {waveform.min().item():6.3f}\")\n",
    "  print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n",
    "  print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n",
    "  print()\n",
    "  print(waveform)\n",
    "  print()\n",
    "\n",
    "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
    "  fig, axs = plt.subplots(1, 1)\n",
    "  axs.set_title(title or 'Spectrogram (db)')\n",
    "  axs.set_ylabel(ylabel)\n",
    "  axs.set_xlabel('frame')\n",
    "  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
    "  if xmax:\n",
    "    axs.set_xlim((0, xmax))\n",
    "  fig.colorbar(im, ax=axs)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "  figure, axes = plt.subplots(num_channels, 1)\n",
    "  if num_channels == 1:\n",
    "    axes = [axes]\n",
    "  for c in range(num_channels):\n",
    "    axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "    axes[c].grid(True)\n",
    "    if num_channels > 1:\n",
    "      axes[c].set_ylabel(f'Channel {c+1}')\n",
    "    if xlim:\n",
    "      axes[c].set_xlim(xlim)\n",
    "    if ylim:\n",
    "      axes[c].set_ylim(ylim)\n",
    "  figure.suptitle(title)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def play_audio(waveform, sample_rate):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  if num_channels == 1:\n",
    "    display(Audio(waveform[0], rate=sample_rate))\n",
    "  elif num_channels == 2:\n",
    "    display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
    "  else:\n",
    "    raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n",
    "\n",
    "# Define a function to plot accuracy and loss\n",
    "\n",
    "def plot_accuracy_loss(training_results): \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(training_results['training_loss'], 'r')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('training loss iterations')\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(training_results['validation_accuracy'])\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epochs')   \n",
    "    plt.show()\n",
    "\n",
    "# Define a function to plot model parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample, test_label = emotional_speech_dataset[1000]\n",
    "print(test_label, test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_waveform(test_sample, 16000)\n",
    "print_stats(test_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 1024\n",
    "win_length = None\n",
    "hop_length = 512\n",
    "\n",
    "# define transformation\n",
    "spectrogram = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    ")\n",
    "# Perform transformation\n",
    "spec = spectrogram(test_sample)\n",
    "\n",
    "print_stats(spec)\n",
    "plot_spectrogram(spec[0], title='torchaudio')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into Train, Test and Validation sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = len(emotional_speech_dataset)\n",
    "train_count = int(0.7 * total_count)\n",
    "valid_count = int(0.2 * total_count)\n",
    "test_count = total_count - train_count - valid_count\n",
    "print('Train count: ' + str(train_count))\n",
    "print('validation count: ' + str(valid_count))\n",
    "print('Test count: ' + str(test_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(emotional_speech_dataset, (train_count, valid_count, test_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(list(set(r[1] for r in train_dataset)))\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_index(word):\n",
    "    # Return the position of the word in labels\n",
    "    return torch.tensor(labels.index(word))\n",
    "\n",
    "\n",
    "def index_to_label(index):\n",
    "    # Return the word corresponding to the index in labels\n",
    "    # This is the inverse of label_to_index\n",
    "    return labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_start = \"neutral\"\n",
    "index = label_to_index(word_start)\n",
    "word_recovered = index_to_label(index)\n",
    "\n",
    "print(word_start, \"-->\", index, \"-->\", word_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_rate = 16000\n",
    "transform = torchaudio.transforms.Resample(orig_freq=16000, new_freq=new_sample_rate)\n",
    "transformed = transform(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = torch.nn.functional.one_hot(index, 12)\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch.permute(0, 2, 1)\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
    "\n",
    "    tensors, targets = [], []\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, label in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label_to_index(label)]\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return tensors, targets\n",
    "\n",
    "batch_size = 64\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=num_workers, pin_memory=pin_memory)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate_fn, num_workers=num_workers, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=8, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M5(n_input=test_sample.shape[0], n_output=len(labels))\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lost_list = []\n",
    "accuracy_list = [ ]\n",
    "N_test=len(test_dataset)\n",
    "\n",
    "\n",
    "def train(model, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        #data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        #data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        # negative log-likelihood for a tensor of size (batch x 1 x n_output)\n",
    "        loss = F.nll_loss(output.squeeze(), target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training stats\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "        # record loss\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        #data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target)\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "        accuracy = correct / N_test\n",
    "        accuracy_list.append(accuracy)     \n",
    "\n",
    "    print(f\"\\nTest Epoch: {epoch}\\tAccuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 20\n",
    "n_epoch = 20\n",
    "\n",
    "pbar_update = 1 / (len(train_loader) + len(test_loader))\n",
    "losses = []\n",
    "\n",
    "# The transform needs to live on the same device as the model and the data.\n",
    "#transform = transform.to(device)\n",
    "with tqdm(total=n_epoch) as pbar:\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        train(model, epoch, log_interval)\n",
    "        test(model, epoch)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.plot(losses, color=color)\n",
    "ax1.set_xlabel('epoch', color=color)\n",
    "ax1.set_ylabel('Loss', color=color)\n",
    "ax1.tick_params(axis='y', color=color)\n",
    "    \n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color) \n",
    "ax2.set_xlabel('epoch', color=color)\n",
    "ax2.plot( accuracy_list, color=color)\n",
    "ax2.tick_params(axis='y', color=color)\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "# Define a function to plot accuracy and loss\n",
    "\n",
    "def plot_accuracy_loss(training_results): \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(training_results['training_loss'], 'r')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('training loss iterations')\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(training_results['validation_accuracy'])\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epochs')   \n",
    "    plt.show()\n",
    "\n",
    "# Define a function to plot model parameters\n",
    "\n",
    "def print_model_parameters(model):\n",
    "    count = 0\n",
    "    for ele in model.state_dict():\n",
    "        count += 1\n",
    "        if count % 2 != 0:\n",
    "            print (\"The following are the parameters for the layer \", count // 2 + 1)\n",
    "        if ele.find(\"bias\") != -1:\n",
    "            print(\"The size of bias: \", model.state_dict()[ele].size())\n",
    "        else:\n",
    "            print(\"The size of weights: \", model.state_dict()[ele].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tensor):\n",
    "    # Use the model to predict the label of the waveform\n",
    "    tensor = tensor.to(device)\n",
    "    tensor = transform(tensor)\n",
    "    tensor = model(tensor.unsqueeze(0))\n",
    "    tensor = get_likely_index(tensor)\n",
    "    tensor = index_to_label(tensor.squeeze())\n",
    "    return tensor\n",
    "\n",
    "\n",
    "waveform, label = train_dataset[-1]\n",
    "#ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
    "\n",
    "print(f\"Expected: {label}. Predicted: {predict(waveform)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3358073929bd3d27ff594bc6528257efc4213b34ba9d7c6bd240dce3a23a83d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('torch_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
