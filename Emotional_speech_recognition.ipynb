{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotional Speech Recognition\n",
    "This notebook will be an audio classification problem and solved with Audio Feature extraction and augmentation, Machine Learning and Deep Learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/dejolilandry/asvpesdspeech-nonspeech-emotional-utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T19:55:11.539525Z",
     "start_time": "2022-01-15T19:54:57.787797Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os \n",
    "import math\n",
    "import librosa\n",
    "import torch \n",
    "import torchaudio \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt \n",
    "#%matplotlib_inline\n",
    "import seaborn as sns\n",
    "import librosa \n",
    "import librosa.display\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "# custom modules \n",
    "from MyDataClasses import ASVPDataset\n",
    "from torch_functions import *\n",
    "import torch_functions as H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T19:55:23.600885Z",
     "start_time": "2022-01-15T19:55:23.561476Z"
    }
   },
   "outputs": [],
   "source": [
    "ASVP_dir = '/Users/stephen/Desktop/Speech_Recognition/Data/ASVP-ESD_UPDATE/Audio/'\n",
    "ASVP_metadata = pd.read_csv('/Users/stephen/Desktop/Speech_Recognition/Data/ASVP-ESD_UPDATE/asvp_metadata.csv', index_col=0)\n",
    "ASVP_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T19:55:25.204164Z",
     "start_time": "2022-01-15T19:55:25.194833Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'The length of the dataset is: {len(ASVP_metadata)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T19:55:26.369718Z",
     "start_time": "2022-01-15T19:55:26.340198Z"
    }
   },
   "outputs": [],
   "source": [
    "ASVP_metadata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deciding how long all the files will be \n",
    "inorder for the CNN to take in an audio dataset all the audio files must be the same length. From the analysis we did in EDA we can see the lengths vary from 3.7 seconds to over 330 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T19:55:28.945743Z",
     "start_time": "2022-01-15T19:55:28.937061Z"
    }
   },
   "outputs": [],
   "source": [
    "# how many are less than 20 seconds \n",
    "over_20_sec = len(ASVP_metadata) - len(ASVP_metadata[ASVP_metadata['Duration'] <= 20])\n",
    "print(f\"There are {over_20_sec} rows over 20 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we only lose 17 rows if we get ride of everything over 20 seconds. Lets see how many we lose when we cut everything over 15 seconds and 10 seconds. The shorter then length of all our audio files the less computation power we'll nee. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T19:55:32.298177Z",
     "start_time": "2022-01-15T19:55:32.289984Z"
    }
   },
   "outputs": [],
   "source": [
    "longest_file = ASVP_metadata[ASVP_metadata['Duration']==ASVP_metadata['Duration'].max()]\n",
    "longest_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T19:55:33.230320Z",
     "start_time": "2022-01-15T19:55:33.227511Z"
    }
   },
   "outputs": [],
   "source": [
    "longest_file_path = f\"{ASVP_dir}actor_3/03-01-05-01-14-03-02-03-01.wav\"\t\n",
    "longest_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T19:55:34.718283Z",
     "start_time": "2022-01-15T19:55:34.409452Z"
    }
   },
   "outputs": [],
   "source": [
    "longest_wav, sr = torchaudio.load(longest_file_path)\n",
    "H.plot_waveform(longest_wav, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T19:55:43.507771Z",
     "start_time": "2022-01-15T19:55:43.475180Z"
    }
   },
   "outputs": [],
   "source": [
    "H.play_audio(longest_wav, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many are less than 15 seconds \n",
    "over_15_sec = len(ASVP_metadata) - len(ASVP_metadata[ASVP_metadata['Duration'] <= 15])\n",
    "print(f\"There are {over_15_sec} rows over 15 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many are less than 15 seconds \n",
    "over_10_sec = len(ASVP_metadata) - len(ASVP_metadata[ASVP_metadata['Duration'] <= 10])\n",
    "print(f\"There are {over_10_sec} rows over 10 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New DataFrame \n",
    "lets decide how long are samples should be but lets look how much it effects the classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20s = ASVP_metadata[ASVP_metadata['Duration'] <= 20]\n",
    "df_15s = ASVP_metadata[ASVP_metadata['Duration'] <= 15]\n",
    "df_10s = ASVP_metadata[ASVP_metadata['Duration'] <= 10]\n",
    "\n",
    "\n",
    "print(f'Length of dataframe (20 seconds): {len(df_20s)} rows')\n",
    "print(f'Length of dataframe (15 seconds): {len(df_15s)} rows')\n",
    "print(f'Length of dataframe (10 seconds): {len(df_10s)} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20s['Emotions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Count of Emotions', size=20)\n",
    "sns.countplot(df_20s.Emotions)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.figure(figsize=(20,56))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Count of Emotions', size=20)\n",
    "sns.countplot(df_15s.Emotions)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.figure(figsize=(20,56))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Count of Emotions', size=20)\n",
    "sns.countplot(df_10s.Emotions)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.figure(figsize=(20,56))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets go with the 10 second dataframe. It will contain the Path's for all the files that are 10 seconds or less. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10s.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10s['Emotions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables \n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "N_FFT = int(0.025 * SAMPLE_RATE)   # 25 ms \n",
    "HOP_LENGTH = int(0.01 * SAMPLE_RATE)  # 10 ms\n",
    "DURATION = 10\n",
    "N_SAMPLES = SAMPLE_RATE * DURATION \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataset = ASVPDataset(\n",
    "    annotations_file=df_10s, \n",
    "    audio_dir=ASVP_dir,\n",
    "    target_sample_rate=SAMPLE_RATE, \n",
    "    num_samples=N_SAMPLES,\n",
    "    device=device  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(audio_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataset[133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprossing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not that we are able to locate our data properly we'll create a custom Dataset object with Pytorch. This will make it easier to work with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample, test_label = audio_dataset[1000]\n",
    "print(test_label, test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_waveform(test_sample, 16000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(test_sample, sample_rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 1024\n",
    "win_length = None\n",
    "hop_length = 512\n",
    "\n",
    "# define transformation\n",
    "spectrogram = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    ")\n",
    "# Perform transformation\n",
    "spec = spectrogram(test_sample)\n",
    "\n",
    "print_stats(spec)\n",
    "plot_spectrogram(spec[0], title='torchaudio')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into Train, Test and Validation sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = len(audio_dataset)\n",
    "train_count = int(0.7 * total_count)\n",
    "valid_count = int(0.2 * total_count)\n",
    "test_count = total_count - train_count - valid_count\n",
    "print('Train count: ' + str(train_count))\n",
    "print('validation count: ' + str(valid_count))\n",
    "print('Test count: ' + str(test_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(audio_dataset, (train_count, valid_count, test_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of entries in training set :', (len(train_dataset)))\n",
    "print('Total number of entries in validation set :', (len(valid_dataset)))\n",
    "print('Total number of entries in test set :', (len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(list(set(r[1] for r in train_dataset)))\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_index(word):\n",
    "    # Return the position of the word in labels\n",
    "    return torch.tensor(labels.index(word))\n",
    "\n",
    "\n",
    "def index_to_label(index):\n",
    "    # Return the word corresponding to the index in labels\n",
    "    # This is the inverse of label_to_index\n",
    "    return labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_start = \"neutral\"\n",
    "index = label_to_index(word_start)\n",
    "word_recovered = index_to_label(index)\n",
    "\n",
    "print(word_start, \"-->\", index, \"-->\", word_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_rate = 8000\n",
    "transform = torchaudio.transforms.Resample(orig_freq=16000, new_freq=new_sample_rate)\n",
    "transformed = transform(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = torch.nn.functional.one_hot(index, 12)\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch.permute(0, 2, 1)\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
    "\n",
    "    tensors, targets = [], []\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, label in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label_to_index(label)]\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return tensors, targets\n",
    "\n",
    "batch_size = 256\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=num_workers, pin_memory=pin_memory)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate_fn, num_workers=num_workers, pin_memory=pin_memory)\n",
    "validation_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate_fn, num_workers=num_workers, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M5 model described in the following paper: \n",
    "# https://arxiv.org/pdf/1610.00087.pdf\n",
    "\n",
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=len(labels), stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M5(n_input=test_sample.shape[0], n_output=len(labels))\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, epoch, log_interval):\n",
    "    model.train()\n",
    "    right = 0\n",
    "    for batch_index, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        pred = get_probable_idx(output)\n",
    "        right += nr_of_right(pred, target)\n",
    "\n",
    "        # negative log-likelihood for a tensor of size (batch x 1 x n_output)\n",
    "        loss = F.nll_loss(output.squeeze(), target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training stats\n",
    "        if batch_index % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_index * len(data)}/{len(train_loader.dataset)} ({100. * batch_index / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\\tAccuracy: {right}/{len(train_loader.dataset)} ({100. * right / len(train_loader.dataset):.0f}%)\")\n",
    "  \n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "        # record loss\n",
    "        losses_train.append(loss.item())\n",
    "\n",
    "    acc = 100. * (right/len(train_loader.dataset))\n",
    "    accuracy_train.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nr_of_right(pred, target):\n",
    "    # count nr of right predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "\n",
    "def get_probable_idx(tensor):\n",
    "    # find most probable wordclass index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, epoch):\n",
    "    #Stop training\n",
    "    model.eval()\n",
    "    \n",
    "    right = 0\n",
    "    for data, target in validation_loader:\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        pred = get_probable_idx(output)\n",
    "        right += nr_of_right(pred, target)\n",
    "\n",
    "        # negative log-likelihood for a tensor of size (batch x 1 x n_output)\n",
    "        loss = F.nll_loss(output.squeeze(), target)\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "\n",
    "    print(f\"\\nValidation Epoch: {epoch} \\tLoss: {loss.item():.6f}\\tAccuracy: {right}/{len(validation_loader.dataset)} ({100. * right / len(validation_loader.dataset):.0f}%)\\n\")\n",
    "     \n",
    "    acc = 100. * right / len(validation_loader.dataset)\n",
    "    accuracy_validation.append(acc)\n",
    "    losses_validation.append(loss.item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    #Stop training\n",
    "    model.eval()\n",
    "    \n",
    "    right = 0\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        pred = get_probable_idx(output)\n",
    "        right += nr_of_right(pred, target)\n",
    "\n",
    "    print(f\"\\nTest set accuracy: {right}/{len(test_loader.dataset)} ({100. * right / len(test_loader.dataset):.0f}%)\\n\")\n",
    "\n",
    "    return (100. * right / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 20\n",
    "n_epoch = 20\n",
    "\n",
    "pbar_update = 1 / (len(train_loader) + len(test_loader))\n",
    "losses_train = []\n",
    "losses_validation = []\n",
    "accuracy_train = []\n",
    "accuracy_validation = []\n",
    "\n",
    "losses = []\n",
    "# The transform needs to live on the same device as the model and the data.\n",
    "transform = transform.to(device)\n",
    "with tqdm(total=n_epoch) as pbar:\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        train(model, epoch, log_interval)\n",
    "        validate(model, epoch)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.plot(losses_train, 'b', label='Train loss')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"M5 model training losses over all iterations\")\n",
    "plt.show()\n",
    "\n",
    "# Plot validation loss\n",
    "plt.plot(losses_validation, 'r', label='Valid loss')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"M5 model validation losses over validation epochs\")\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(accuracy_train, 'b', label='Train acc')\n",
    "plt.plot(accuracy_validation,'r', label ='Valid acc')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"M5 model training & validation accuracy over epochs\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tensor):\n",
    "    # Use the model to predict the label of the waveform\n",
    "    tensor = tensor.to(device)\n",
    "    tensor = transform(tensor)\n",
    "    tensor = model(tensor.unsqueeze(0))\n",
    "    tensor = get_likely_index(tensor)\n",
    "    tensor = index_to_label(tensor.squeeze())\n",
    "    return tensor\n",
    "\n",
    "\n",
    "waveform, label = train_dataset[-1]\n",
    "#ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
    "\n",
    "print(f\"Expected: {label}. Predicted: {predict(waveform)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3358073929bd3d27ff594bc6528257efc4213b34ba9d7c6bd240dce3a23a83d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
